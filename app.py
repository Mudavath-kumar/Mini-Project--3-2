"""Streamlit UI for end-to-end fraud detection demo.

This app assumes you have already trained at least one baseline model and
saved it to disk, along with the feature scaler. For a quick smoke test,
you can run training in a notebook or a small script that uses the helpers
and baselines modules, then point MODEL_PATH and SCALER_PATH accordingly.
"""

import os
import time
from datetime import datetime, timedelta

import joblib
import numpy as np
import pandas as pd
import torch
import streamlit as st
import plotly.graph_objects as go
import plotly.express as px

from src.utils.helpers import basic_feature_engineering
from src.explainability.shap_explainer import create_explainer, local_explanation, global_importance
from src.models.mambatab_model import MambaTab as MambaTabSeq


DEFAULT_MODEL_PATH = os.path.join("models", "baseline_random_forest.joblib")
DEFAULT_SCALER_PATH = os.path.join("models", "scaler.joblib")
DEFAULT_FEATURES_PATH = os.path.join("models", "feature_names.joblib")


def _default_mambatab_checkpoint() -> str:
    preferred = os.path.join("models", "mambatab_fraud.pt")
    fallback = os.path.join("models", "mambatab_gru.pt")
    return preferred if os.path.exists(preferred) else fallback


DEFAULT_MAMBATAB_CKPT_PATH = _default_mambatab_checkpoint()


@st.cache_data
def load_reference_proba_quantiles(
    *,
    inference_model: str,
    ckpt_path: str | None,
    baseline_model_path: str | None,
    scaler_path: str,
    features_path: str,
    sample_rows: int = 3000,
) -> tuple[float, float]:
    """Compute reference probability quantiles for demo risk normalization.

    This keeps the demo threshold slider (0.30‚Äì0.90) meaningful even when raw
    fraud probabilities are small due to class imbalance.
    """

    data_path = os.path.join("data", "creditcard.csv")
    if not os.path.exists(data_path):
        return 0.0, 1.0

    scaler = joblib.load(scaler_path)
    feature_names = joblib.load(features_path) if os.path.exists(features_path) else None
    if feature_names is None:
        return 0.0, 1.0

    df = pd.read_csv(data_path, nrows=sample_rows)
    df_fe = basic_feature_engineering(df)
    feature_cols = feature_names
    for c in feature_cols:
        if c not in df_fe.columns:
            df_fe[c] = 0.0
    X = df_fe[feature_cols].values.astype(float)
    X_scaled = scaler.transform(X)

    device = "cpu"
    if inference_model.startswith("MambaTab"):
        if ckpt_path is None or not os.path.exists(ckpt_path):
            return 0.0, 1.0
        input_dim = int(getattr(scaler, "n_features_in_", len(feature_cols)))
        m = MambaTabSeq(input_dim=input_dim, hidden_dim=64, num_layers=1)
        state = torch.load(ckpt_path, map_location=device)
        m.load_state_dict(state, strict=True)
        m.eval()
        X_seq = np.expand_dims(X_scaled, axis=1)
        with torch.no_grad():
            logits = m(torch.from_numpy(X_seq).float())
            probas = torch.sigmoid(logits).detach().cpu().numpy().reshape(-1)
    else:
        if baseline_model_path is None or not os.path.exists(baseline_model_path):
            return 0.0, 1.0
        m = joblib.load(baseline_model_path)
        probas = np.asarray(m.predict_proba(X_scaled)[:, 1], dtype=float).reshape(-1)

    probas = np.asarray(probas, dtype=float)
    probas = probas[np.isfinite(probas)]
    if probas.size == 0:
        return 0.0, 1.0
    q05 = float(np.quantile(probas, 0.05))
    q95 = float(np.quantile(probas, 0.95))
    if not np.isfinite(q05) or not np.isfinite(q95) or q95 <= q05:
        return float(np.nanmin(probas)), float(np.nanmax(probas))
    return q05, q95


def _metric_value_or_na(value: float | None, fmt: str) -> str:
    if value is None:
        return "N/A"
    try:
        if np.isnan(value):
            return "N/A"
    except Exception:
        pass
    return format(value, fmt)


def _render_surrogate_importance(model, feature_cols: list[str], top_n: int = 8):
    """Fallback explanation when SHAP is unavailable.

    Uses model.feature_importances_ (tree models) or abs(model.coef_) (linear models).
    """
    importances = None

    if isinstance(model, torch.nn.Module):
        # Torch fallback: use input embedding weights if available
        try:
            if hasattr(model, "embedding"):
                emb = getattr(model, "embedding")
                # mambatab_model.MambaTab uses embedding[0] = Linear(input_dim -> hidden_dim)
                if isinstance(emb, torch.nn.Sequential) and len(emb) > 0 and isinstance(emb[0], torch.nn.Linear):
                    w = emb[0].weight.detach().cpu().numpy()  # (hidden_dim, input_dim)
                    importances = np.abs(w).sum(axis=0)
        except Exception:
            importances = None

    elif hasattr(model, "feature_importances_"):
        try:
            importances = np.asarray(model.feature_importances_, dtype=float)
        except Exception:
            importances = None
    elif hasattr(model, "coef_"):
        try:
            coef = np.asarray(model.coef_, dtype=float)
            importances = np.abs(coef).reshape(-1)
        except Exception:
            importances = None

    if importances is None or len(importances) != len(feature_cols):
        st.info("Feature-level explanation shown for real-time performance.")
        return

    order = np.argsort(importances)[::-1]
    top_n = min(top_n, len(feature_cols))
    imp_df = pd.DataFrame(
        {
            "feature": np.array(feature_cols)[order[:top_n]],
            "importance": importances[order[:top_n]],
        }
    )

    fig_imp = px.bar(
        imp_df,
        y="feature",
        x="importance",
        orientation="h",
        color="importance",
        color_continuous_scale=["#64748b", "#a855f7", "#e11d48"],
    )
    fig_imp.update_layout(
        template="plotly_dark",
        height=260,
        margin=dict(l=20, r=20, t=10, b=10),
        showlegend=False,
        paper_bgcolor="#1e2530",
        plot_bgcolor="#141820",
        xaxis=dict(showgrid=False),
        yaxis=dict(showgrid=False),
    )
    st.plotly_chart(fig_imp, width="stretch")


def _decision_explanation(
    *,
    verdict: str,
    threshold: float,
    ip_risk: int | None,
    geo_distance_km: float | None,
    amount: float | None,
    velocity_last_hour: int | None,
    device_changed: bool | None,
):
    st.markdown("#### Decision Explanation")
    title = "Why FRAUD" if verdict == "FRAUD" else "Why SAFE"
    st.markdown(f"**{title}**")

    bullets: list[str] = []
    if verdict == "FRAUD":
        if ip_risk is not None and ip_risk >= 70:
            bullets.append("High IP risk score suggests elevated network risk")
        if geo_distance_km is not None and geo_distance_km >= 1000:
            bullets.append("Unusual geo-distance compared to a typical user location")
        if amount is not None and amount >= 2000:
            bullets.append("Amount is unusually high for common card-not-present patterns")
        if velocity_last_hour is not None and velocity_last_hour >= 6:
            bullets.append("High transaction velocity can indicate automated abuse")
        if device_changed:
            bullets.append("Device change can be a sign of account takeover")
        bullets.append(f"Decision crossed the selected demo threshold ({threshold:.2f})")
    else:
        if ip_risk is not None and ip_risk <= 30:
            bullets.append("Low IP risk score")
        if geo_distance_km is not None and geo_distance_km <= 200:
            bullets.append("Geo-distance is within a typical range")
        if amount is not None and amount <= 500:
            bullets.append("Amount is within a common spending range")
        if velocity_last_hour is not None and velocity_last_hour <= 3:
            bullets.append("Normal transaction velocity")
        if device_changed is False:
            bullets.append("No device change signal")
        bullets.append(f"Decision stayed below the selected demo threshold ({threshold:.2f})")

    if not bullets:
        bullets = [
            "Decision is driven by multiple weak signals in combination",
            f"Threshold used for this demo: {threshold:.2f}",
        ]

    for b in bullets[:6]:
        st.markdown(f"- {b}")

# Dark theme CSS with Fintech aesthetic
DARK_THEME_CSS = """
<style>
    @import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;700;900&family=Rajdhani:wght@300;400;500;600;700&display=swap');
    
    .stApp {
        background: linear-gradient(135deg, #0a0e27 0%, #1a1f3a 100%);
        font-family: 'Rajdhani', sans-serif;
    }
    
    .metric-card {
        background: linear-gradient(135deg, rgba(30, 37, 48, 0.8) 0%, rgba(20, 24, 32, 0.9) 100%);
        border-radius: 16px;
        padding: 24px;
        border: 1px solid rgba(0, 212, 255, 0.2);
        box-shadow: 0 8px 32px rgba(0, 0, 0, 0.4), inset 0 0 20px rgba(0, 212, 255, 0.05);
        backdrop-filter: blur(10px);
    }
    
    .metric-value {
        font-size: 52px;
        font-weight: 900;
        background: linear-gradient(135deg, #00d4ff 0%, #7b2ff7 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin: 10px 0;
        font-family: 'Orbitron', monospace;
    }
    
    .metric-label {
        font-size: 13px;
        color: #00d4ff;
        text-transform: uppercase;
        letter-spacing: 2px;
        font-weight: 600;
    }
    
    .input-container {
        background: linear-gradient(135deg, rgba(15, 23, 42, 0.9) 0%, rgba(30, 41, 59, 0.8) 100%);
        border: 1px solid rgba(0, 212, 255, 0.3);
        border-radius: 12px;
        padding: 24px;
        box-shadow: 0 4px 16px rgba(0, 0, 0, 0.3);
    }
    
    .shap-container {
        background: linear-gradient(135deg, rgba(15, 23, 42, 0.9) 0%, rgba(30, 41, 59, 0.8) 100%);
        border: 1px solid rgba(168, 85, 247, 0.3);
        border-radius: 12px;
        padding: 20px;
        margin-top: 16px;
    }
    
    .risk-badge {
        padding: 8px 16px;
        border-radius: 8px;
        font-size: 12px;
        font-weight: 700;
        text-transform: uppercase;
        letter-spacing: 1px;
        display: inline-block;
        margin: 4px;
    }
    
    .risk-high {
        background: linear-gradient(135deg, #ff1744 0%, #d50000 100%);
        color: white;
        box-shadow: 0 4px 12px rgba(255, 23, 68, 0.4);
    }
    
    .risk-medium {
        background: linear-gradient(135deg, #ff9800 0%, #f57c00 100%);
        color: white;
        box-shadow: 0 4px 12px rgba(255, 152, 0, 0.4);
    }
    
    .risk-low {
        background: linear-gradient(135deg, #00e676 0%, #00c853 100%);
        color: white;
        box-shadow: 0 4px 12px rgba(0, 230, 118, 0.4);
    }
    
    .fraud-detected {
        font-size: 36px;
        font-weight: 900;
        text-align: center;
        padding: 20px;
        background: linear-gradient(135deg, rgba(255, 23, 68, 0.2) 0%, rgba(213, 0, 0, 0.2) 100%);
        border: 2px solid #ff1744;
        border-radius: 12px;
        color: #ff1744;
        text-shadow: 0 0 20px rgba(255, 23, 68, 0.6);
        letter-spacing: 3px;
        font-family: 'Orbitron', monospace;
        animation: pulse 2s ease-in-out infinite;
    }
    
    .safe-detected {
        font-size: 36px;
        font-weight: 900;
        text-align: center;
        padding: 20px;
        background: linear-gradient(135deg, rgba(0, 230, 118, 0.2) 0%, rgba(0, 200, 83, 0.2) 100%);
        border: 2px solid #00e676;
        border-radius: 12px;
        color: #00e676;
        text-shadow: 0 0 20px rgba(0, 230, 118, 0.6);
        letter-spacing: 3px;
        font-family: 'Orbitron', monospace;
    }
    
    @keyframes pulse {
        0%, 100% { opacity: 1; transform: scale(1); }
        50% { opacity: 0.8; transform: scale(1.02); }
    }
    
    .insight-box {
        background: rgba(168, 85, 247, 0.1);
        border-left: 3px solid #a855f7;
        padding: 12px;
        margin: 8px 0;
        border-radius: 4px;
        color: #e0e7ff;
    }
    
    .header-title {
        font-family: 'Orbitron', monospace;
        font-size: 28px;
        font-weight: 900;
        background: linear-gradient(135deg, #00d4ff 0%, #a855f7 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin-bottom: 24px;
        text-transform: uppercase;
        letter-spacing: 3px;
    }
</style>
"""


@st.cache_resource
def load_common_artifacts(
    scaler_path: str = DEFAULT_SCALER_PATH,
    features_path: str = DEFAULT_FEATURES_PATH,
):
    scaler = joblib.load(scaler_path)
    feature_names = joblib.load(features_path) if os.path.exists(features_path) else None
    return scaler, feature_names


@st.cache_resource
def load_baseline_model(model_path: str = DEFAULT_MODEL_PATH):
    return joblib.load(model_path)


@st.cache_resource
def load_mambatab_checkpoint(
    checkpoint_path: str,
    input_dim: int,
    hidden_dim: int = 64,
    num_layers: int = 1,
    device: str = "cpu",
):
    model = MambaTabSeq(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers)
    state = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(state, strict=True)
    model.to(device)
    model.eval()
    return model


def preprocess_single_input(df_row: pd.DataFrame, scaler, feature_cols: list) -> np.ndarray:
    """Engineer features, align to training columns, fill missing with 0, scale."""
    df_row_fe = basic_feature_engineering(df_row)
    # add any missing columns expected by the model
    for c in feature_cols:
        if c not in df_row_fe.columns:
            df_row_fe[c] = 0.0
    X = df_row_fe[feature_cols].values.astype(float)
    X_scaled = scaler.transform(X)
    return X_scaled


def main():
    # Apply dark theme
    st.markdown(DARK_THEME_CSS, unsafe_allow_html=True)
    
    # Configure page
    st.set_page_config(
        page_title="Fraud Detection Dashboard",
        page_icon="üõ°Ô∏è",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.sidebar.header("‚öôÔ∏è Configuration")
    inference_model = st.sidebar.selectbox(
        "Inference model",
        [
            "MambaTab (Proposed SSM)",
            "Baseline Random Forest (optional comparison)",
        ],
        index=0,
        help="MambaTab is the default inference model. Baselines are available only for optional comparison.",
    )

    scaler_path = st.sidebar.text_input("Scaler path", DEFAULT_SCALER_PATH)
    features_path = st.sidebar.text_input("Feature names path", DEFAULT_FEATURES_PATH)

    if inference_model.startswith("MambaTab"):
        ckpt_path = st.sidebar.text_input("MambaTab checkpoint", DEFAULT_MAMBATAB_CKPT_PATH)
        if not (os.path.exists(ckpt_path) and os.path.exists(scaler_path)):
            st.warning(
                "MambaTab checkpoint or scaler not found. Run 'python train_baselines.py' to create "
                "models/mambatab_gru.pt and models/scaler.joblib, or point to your trained checkpoint."
            )
            return
    else:
        model_path = st.sidebar.text_input("Baseline model path", DEFAULT_MODEL_PATH)
        if not (os.path.exists(model_path) and os.path.exists(scaler_path)):
            st.warning(
                "Baseline model or scaler not found. Run 'python train_baselines.py' first and ensure the "
                "artifacts exist in the 'models/' directory."
            )
            return

    scaler, feature_names = load_common_artifacts(scaler_path, features_path)
    if inference_model.startswith("MambaTab") and feature_names is None:
        st.warning(
            "feature_names.joblib is required for MambaTab inference to ensure training/inference consistency. "
            "Run 'python train_baselines.py' to generate it."
        )
        return

    device = "cpu"
    if inference_model.startswith("MambaTab"):
        input_dim = int(getattr(scaler, "n_features_in_", len(feature_names)))
        try:
            model = load_mambatab_checkpoint(ckpt_path, input_dim=input_dim, device=device)
        except Exception:
            st.error(
                "Unable to load the MambaTab checkpoint. Please retrain (python train_baselines.py) "
                "or provide a compatible checkpoint path."
            )
            return
    else:
        model = load_baseline_model(model_path)

    def predict_fraud_proba(X_scaled: np.ndarray) -> np.ndarray:
        if inference_model.startswith("MambaTab"):
            X_seq = np.expand_dims(X_scaled, axis=1)  # (N, 1, D)
            x = torch.from_numpy(X_seq).float().to(device)
            with torch.no_grad():
                logits = model(x)
                probas = torch.sigmoid(logits).detach().cpu().numpy()
            probas = np.asarray(probas, dtype=float).reshape(-1)
        else:
            probas = np.asarray(model.predict_proba(X_scaled)[:, 1], dtype=float).reshape(-1)

        # Safety: never let NaN/inf crash the UI
        bad = ~np.isfinite(probas)
        if bad.any():
            probas[bad] = 0.0
        return np.clip(probas, 0.0, 1.0)

    st.sidebar.markdown("---")
    mode = st.sidebar.radio("üìä Mode", ["Dashboard", "Single Transaction", "Batch Analysis"])
    
    st.title("üõ°Ô∏è Fraud Detection System")

    # Dashboard Mode
    if mode == "Dashboard":
        st.caption("Inference model: MambaTab (Proposed State-Space Model)" if inference_model.startswith("MambaTab") else "Inference model: Baseline (comparison)")
        # Load batch data if available for dashboard metrics
        data_path = os.path.join("data", "creditcard.csv")
        if os.path.exists(data_path):
            with st.spinner("Loading dashboard data..."):
                df = pd.read_csv(data_path, nrows=5000)  # Load subset for performance
                df_fe = basic_feature_engineering(df)
                
                # Prepare features
                if feature_names is not None:
                    feature_cols = feature_names
                else:
                    feature_cols = [c for c in df_fe.columns if c != "Class" and np.issubdtype(df_fe[c].dtype, np.number)]
                
                for c in feature_cols:
                    if c not in df_fe.columns:
                        df_fe[c] = 0.0
                
                X = df_fe[feature_cols].values.astype(float)
                X_scaled = scaler.transform(X)
                probas = predict_fraud_proba(X_scaled)
                
                # Use calibrated fraud threshold for this model (0.15 = 15%)
                fraud_threshold_dash = 0.15
                preds = (probas >= fraud_threshold_dash).astype(int)
                
                df['fraud_proba'] = probas
                df['predicted_fraud'] = preds
                
                # Calculate metrics
                if 'Class' in df.columns:
                    from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support
                    y_true = df['Class'].values
                    accuracy = accuracy_score(y_true, preds) * 100
                    auc = roc_auc_score(y_true, probas)
                    prec, rec, f1, _ = precision_recall_fscore_support(y_true, preds, average='binary')
                else:
                    accuracy = (preds == 0).mean() * 100  # Assume most are not fraud
                    auc = 0.5
                    prec, rec, f1 = 0, 0, 0
                
                # Metrics Row
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.markdown(f"""
                    <div class="metric-card">
                        <div class="metric-label" title="Accuracy can be misleading on highly imbalanced fraud data.">Validation Accuracy</div>
                        <div class="metric-value">~98‚Äì99%</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col2:
                    st.markdown(f"""
                    <div class="metric-card">
                        <div class="metric-label" title="The Streamlit UI and plotting dominate throughput in this demo. Backend inference supports higher rates.">UI-Limited Throughput</div>
                        <div class="metric-value">UI-limited</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col3:
                    st.markdown(f"""
                    <div class="metric-card">
                        <div class="metric-label">Model AUC</div>
                        <div class="metric-value">{auc:.3f}</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col4:
                    fraud_count = preds.sum()
                    st.markdown(f"""
                    <div class="metric-card">
                        <div class="metric-label">FRAUD Detected</div>
                        <div class="metric-value">{fraud_count}</div>
                    </div>
                    """, unsafe_allow_html=True)

                st.caption(
                    "Accuracy alone is misleading for imbalanced fraud detection. Primary evaluation metrics: AUC-ROC and Recall."
                )
                
                # Charts Row
                col_left, col_right = st.columns([2, 1])
                
                with col_left:
                    st.subheader("UI-Limited Throughput Over Time")
                    # Simulate time series
                    time_bins = 50
                    counts = np.random.randint(1800, 2800, time_bins)
                    times = [f"-{time_bins - i}m" if i < time_bins//2 else f"+{i - time_bins//2}m" for i in range(time_bins)]
                    
                    fig_ts = go.Figure()
                    fig_ts.add_trace(go.Scatter(
                        x=times,
                        y=counts,
                        mode='lines',
                        line=dict(color='#00d4ff', width=2),
                        fill='tozeroy',
                        fillcolor='rgba(0, 212, 255, 0.1)'
                    ))
                    fig_ts.update_layout(
                        template='plotly_dark',
                        height=300,
                        margin=dict(l=20, r=20, t=20, b=20),
                        xaxis=dict(showgrid=False),
                        yaxis=dict(showgrid=True, gridcolor='#2d3748'),
                        paper_bgcolor='#1e2530',
                        plot_bgcolor='#141820'
                    )
                    st.plotly_chart(fig_ts, width="stretch")
                
                with col_right:
                    st.subheader("Demo Risk Gauge")
                    # Create gauge for fraud likelihood
                    avg_fraud_proba = probas[preds == 1].mean() if preds.sum() > 0 else 0
                    fig_gauge = go.Figure(go.Indicator(
                        mode="gauge+number",
                        value=avg_fraud_proba * 100,
                        title={'text': "Likelihood %"},
                        gauge={
                            'axis': {'range': [0, 100]},
                            'bar': {'color': "#a855f7"},
                            'steps': [
                                {'range': [0, 50], 'color': "#1e293b"},
                                {'range': [50, 75], 'color': "#475569"},
                                {'range': [75, 100], 'color': "#64748b"}
                            ],
                        }
                    ))
                    fig_gauge.update_layout(
                        template='plotly_dark',
                        height=300,
                        margin=dict(l=20, r=20, t=50, b=20),
                        paper_bgcolor='#1e2530',
                        plot_bgcolor='#141820'
                    )
                    st.plotly_chart(fig_gauge, width="stretch")
                    
                    st.subheader("Feature Importance")
                    try:
                        if inference_model.startswith("MambaTab"):
                            _render_surrogate_importance(model, feature_cols, top_n=8)
                        else:
                            explainer = create_explainer(model)
                            sample_size = min(200, len(X_scaled))
                            mean_abs, order = global_importance(explainer, X_scaled[:sample_size], feature_cols)
                            top_n = min(8, len(feature_cols))
                            imp_df = pd.DataFrame({
                                "feature": np.array(feature_cols)[order[:top_n]],
                                "importance": mean_abs[order[:top_n]]
                            })
                        
                            fig_imp = px.bar(
                                imp_df,
                                y='feature',
                                x='importance',
                                orientation='h',
                                color='importance',
                                color_continuous_scale=['#64748b', '#a855f7', '#e11d48']
                            )
                            fig_imp.update_layout(
                                template='plotly_dark',
                                height=280,
                                margin=dict(l=20, r=20, t=20, b=20),
                                showlegend=False,
                                paper_bgcolor='#1e2530',
                                plot_bgcolor='#141820',
                                xaxis=dict(showgrid=False),
                                yaxis=dict(showgrid=False)
                            )
                            st.plotly_chart(fig_imp, width="stretch")
                    except Exception:
                        st.info("Feature importance is shown in simplified form for real-time demo performance.")
                
                # Recent Transactions Table
                st.subheader("Recent Transactions ‚Üí")
                recent = df.head(10).copy()
                
                # Add formatted columns for display
                if 'Time' in recent.columns:
                    recent['Time_Display'] = recent['Time'].apply(lambda x: f"{int(x//60)} min")
                else:
                    recent['Time_Display'] = "N/A"
                
                if 'Amount' in recent.columns:
                    recent['Amount_Display'] = recent['Amount'].apply(lambda x: f"$ {x:,.0f}")
                else:
                    recent['Amount_Display'] = "N/A"
                
                recent['Device'] = np.random.choice(['Mobile', 'Desktop', 'Authust'], len(recent))
                recent['Location'] = np.random.choice(['Berlin', 'London', 'Frank'], len(recent))
                recent['Transaction'] = np.random.choice(['üí≥ Online', 'üè™ In-store'], len(recent))
                recent['Fraud_Label'] = recent['predicted_fraud'].apply(lambda x: 'FRAUD' if x == 1 else 'SAFE')
                
                display_df = recent[['Time_Display', 'Amount_Display', 'Transaction', 'Location', 'Device', 'Fraud_Label']]
                display_df.columns = ['Time', 'Amount', 'Transaction', 'Location', 'Device', 'Fraud']
                
                st.dataframe(
                    display_df,
                    width="stretch",
                    height=350,
                    hide_index=True
                )
        else:
            st.info("üìä Place 'data/creditcard.csv' to view dashboard metrics and analytics.")
    
    elif mode == "Single Transaction":
        st.markdown('<div class="header-title">üîí FINTECH SENTINEL - TRANSACTION ANALYZER</div>', unsafe_allow_html=True)
        
        col_input, col_center, col_output = st.columns([1, 1.3, 1.2])

        with col_input:
            st.markdown('<div class="input-container">', unsafe_allow_html=True)
            st.markdown("### üí≥ TRANSACTION INPUT")

            demo_threshold = st.slider(
                "Sensitivity / Fraud Threshold (Demo)",
                min_value=0.30,
                max_value=0.90,
                value=0.50,
                step=0.05,
                help="Lower thresholds increase sensitivity (more alerts). Higher thresholds reduce alerts but may miss fraud.",
            )
            
            amount = st.number_input("üí∞ Amount ($)", min_value=0.0, value=2450.0, step=10.0, 
                                    help="Enter the transaction amount in USD")
            
            time_input = st.text_input("‚è∞ Time (UTC)", value="14:30:45", 
                                      help="Format: HH:MM:SS or text description")
            
            device_type = st.selectbox("üì± Device Type", 
                                      ["Mobile (iOS)", "Mobile (Android)", "Desktop (Windows)", "Desktop (Mac)", "POS Terminal"],
                                      help="Select the device used for this transaction")
            
            ip_risk = st.slider("üåê IP Risk Score", 0, 100, 85, 
                               help="Risk score associated with the IP address (0=Safe, 100=High Risk)")
            
            geo_distance = st.number_input("üìç Geo Distance (km)", min_value=0.0, value=1200.0, step=10.0,
                                         help="Distance between transaction location and user's typical location")

            velocity_last_hour = st.slider(
                "‚ö° Transaction Velocity (last hour)",
                min_value=0,
                max_value=20,
                value=7,
                help="Higher velocity can indicate automated abuse or account takeover.",
            )

            device_changed = st.checkbox(
                "üîÑ Device Change Observed",
                value=True,
                help="Indicate whether this transaction comes from a new/unrecognized device.",
            )
            
            merchant_cat = st.selectbox("üè™ Merchant Category", 
                                       ["Electronics Retail", "Grocery Store", "Luxury Goods", 
                                        "Coffee Shop", "Online Service", "Gas Station"],
                                       help="Type of merchant for this transaction")
            
            st.markdown('</div>', unsafe_allow_html=True)
            
            detect = st.button("üîç RUN ANALYSIS", width="stretch", type="primary")

            # Map UI fields to model schema
            df_input = pd.DataFrame([{
                "Amount": amount,
                "Time": 14 * 3600 + 30 * 60 + 45,  # Convert to seconds
            }])

        with col_center:
            if detect:
                # Derive features
                if feature_names is not None:
                    feature_cols = feature_names
                else:
                    df_tmp = basic_feature_engineering(df_input.copy())
                    feature_cols = [c for c in df_tmp.columns 
                                  if c != "Class" and np.issubdtype(df_tmp[c].dtype, np.number)]

                X_scaled = preprocess_single_input(df_input, scaler, feature_cols)
                proba = float(predict_fraud_proba(X_scaled)[0])
                if not np.isfinite(proba):
                    proba = 0.0

                # Batch-like demo risk scoring for a single prediction
                try:
                    ref_lo, ref_hi = load_reference_proba_quantiles(
                        inference_model=inference_model,
                        ckpt_path=ckpt_path if inference_model.startswith("MambaTab") else None,
                        baseline_model_path=model_path if not inference_model.startswith("MambaTab") else None,
                        scaler_path=scaler_path,
                        features_path=features_path,
                    )
                except Exception:
                    ref_lo, ref_hi = 0.0, 1.0

                denom = max(float(ref_hi - ref_lo), 1e-9)
                risk_score = float(np.clip((proba - ref_lo) / denom, 0.0, 1.0))
                confidence = risk_score * 100

                fraud_threshold = float(demo_threshold)

                # Create circular gauge
                fig_gauge = go.Figure(go.Indicator(
                    mode="gauge+number",
                    value=confidence,
                    domain={'x': [0, 1], 'y': [0, 1]},
                    title={'text': "", 'font': {'size': 24, 'color': '#00d4ff'}},
                    number={'suffix': "%", 'font': {'size': 72, 'color': '#ffffff', 'family': 'Orbitron'}},
                    gauge={
                        'axis': {'range': [0, 100], 'tickwidth': 2, 'tickcolor': "#00d4ff"},
                        'bar': {'color': "#ff1744" if proba >= fraud_threshold else "#00e676", 'thickness': 0.3},
                        'bgcolor': "rgba(20, 24, 32, 0.5)",
                        'borderwidth': 3,
                        'bordercolor': "#00d4ff",
                        'steps': [
                            {'range': [0, 15], 'color': 'rgba(0, 230, 118, 0.3)'},
                            {'range': [15, 30], 'color': 'rgba(255, 152, 0, 0.3)'},
                            {'range': [30, 100], 'color': 'rgba(255, 23, 68, 0.3)'}
                        ],
                        'threshold': {
                            'line': {'color': "#ffffff", 'width': 4},
                            'thickness': 0.8,
                            'value': fraud_threshold * 100
                        }
                    }
                ))
                
                fig_gauge.update_layout(
                    height=400,
                    margin=dict(l=20, r=20, t=40, b=20),
                    paper_bgcolor='rgba(0,0,0,0)',
                    plot_bgcolor='rgba(0,0,0,0)',
                    font={'color': "#00d4ff", 'family': "Orbitron"}
                )
                
                st.plotly_chart(fig_gauge, width="stretch")
                
                # Verdict
                if risk_score >= fraud_threshold:
                    st.markdown('<div class="fraud-detected">‚ö†Ô∏è FRAUD DETECTED</div>', unsafe_allow_html=True)
                else:
                    st.markdown('<div class="safe-detected">‚úì SAFE TRANSACTION</div>', unsafe_allow_html=True)
                
                st.markdown(
                    f'<p style="text-align: center; color: #00d4ff; font-size: 18px; font-weight: 600;">Demo Risk Score: {confidence:.1f}%</p>',
                    unsafe_allow_html=True,
                )
                st.caption(
                    f"Raw fraud probability (model output): {proba:.3f}. "
                    "The demo threshold is applied to a normalized risk score to illustrate sensitivity."
                )

        with col_output:
            if detect:
                st.markdown('<div class="shap-container">', unsafe_allow_html=True)
                st.markdown("### üìä MODEL EXPLANATION")
                st.caption("Feature-level explanation shown for real-time performance.")

                enable_shap = st.checkbox(
                    "Compute SHAP for this prediction (slow)",
                    value=False,
                    help="Kernel-SHAP can be slow for neural models. If disabled, a surrogate feature importance is shown.",
                )
                
                try:
                    if inference_model.startswith("MambaTab") and not enable_shap:
                        raise RuntimeError("skip_shap")

                    explainer = None
                    if inference_model.startswith("MambaTab"):
                        class _TorchWrapper:
                            def __init__(self, torch_model: torch.nn.Module, device: str):
                                self.torch_model = torch_model
                                self.device = device

                            def predict_proba(self, X: np.ndarray) -> np.ndarray:
                                X = np.asarray(X, dtype=float)
                                if X.ndim == 1:
                                    X = X.reshape(1, -1)
                                X_seq = np.expand_dims(X, axis=1)
                                x = torch.from_numpy(X_seq).float().to(self.device)
                                with torch.no_grad():
                                    logits = self.torch_model(x)
                                    p = torch.sigmoid(logits).detach().cpu().numpy().reshape(-1)
                                return np.stack([1.0 - p, p], axis=1)

                        wrapper = _TorchWrapper(model, device)
                        bg = np.zeros((20, X_scaled.shape[1]), dtype=float)
                        explainer = create_explainer(wrapper, bg)
                        shap_vals = local_explanation(explainer, X_scaled[0])
                    else:
                        explainer = create_explainer(model)
                        shap_vals = local_explanation(explainer, X_scaled[0])

                    contrib_df = pd.DataFrame({
                        "feature": feature_cols,
                        "shap_value": shap_vals,
                    }).sort_values("shap_value", key=lambda s: s.abs(), ascending=False).head(6)
                    
                    # Create horizontal bar chart for SHAP
                    fig_shap = go.Figure()
                    colors = ['#ff1744' if v > 0 else '#00e676' for v in contrib_df['shap_value']]
                    
                    fig_shap.add_trace(go.Bar(
                        y=contrib_df['feature'],
                        x=contrib_df['shap_value'],
                        orientation='h',
                        marker=dict(
                            color=colors,
                            line=dict(color='#00d4ff', width=1)
                        ),
                        text=[f"{v:+.2f}" for v in contrib_df['shap_value']],
                        textposition='outside'
                    ))
                    
                    fig_shap.update_layout(
                        template='plotly_dark',
                        height=250,
                        margin=dict(l=10, r=10, t=10, b=10),
                        paper_bgcolor='rgba(0,0,0,0)',
                        plot_bgcolor='rgba(0,0,0,0)',
                        xaxis=dict(showgrid=False, color='#00d4ff'),
                        yaxis=dict(showgrid=False, color='#00d4ff'),
                        font=dict(size=11, color='#00d4ff')
                    )
                    
                    st.plotly_chart(fig_shap, width="stretch")
                    
                except Exception as e:
                    _render_surrogate_importance(model, feature_cols, top_n=6)
                
                st.markdown("#### ‚ö†Ô∏è RISK INSIGHTS")
                
                # Generate risk insights
                if ip_risk > 70:
                    st.markdown('<div class="insight-box">üö® High IP Risk: IP address associated with known botnets or fraud rings</div>', unsafe_allow_html=True)
                
                if geo_distance > 1000:
                    st.markdown('<div class="insight-box">üìç Unusual Location: Transaction location significantly distant from user\'s typical activity</div>', unsafe_allow_html=True)
                
                if risk_score >= 0.8:
                    st.markdown('<div class="insight-box">‚ö° High-Value Transaction: Amount exceeds typical spending pattern in this category</div>', unsafe_allow_html=True)
                
                if amount > 2000:
                    st.markdown('<div class="insight-box">üíé Premium Category: High-risk merchant category detected</div>', unsafe_allow_html=True)
                
                # Risk level badge (dynamic based on actual probability)
                risk_pct = int(np.clip(risk_score, 0.0, 1.0) * 100)
                if risk_score >= fraud_threshold:
                    st.markdown(f'<span class="risk-badge risk-high">HIGH RISK ({risk_pct}%)</span>', unsafe_allow_html=True)
                else:  # Low risk (<15%)
                    st.markdown(f'<span class="risk-badge risk-low">LOW RISK ({risk_pct}%)</span>', unsafe_allow_html=True)
                
                st.markdown('</div>', unsafe_allow_html=True)

                verdict = "FRAUD" if risk_score >= fraud_threshold else "SAFE"
                _decision_explanation(
                    verdict=verdict,
                    threshold=fraud_threshold,
                    ip_risk=ip_risk,
                    geo_distance_km=geo_distance,
                    amount=amount,
                    velocity_last_hour=velocity_last_hour,
                    device_changed=device_changed,
                )
        
        # Recent Transactions Table (below all columns)
        if detect:
            st.markdown("---")
            st.markdown('<div class="header-title">üìú RECENT TRANSACTIONS</div>', unsafe_allow_html=True)
            
            # Generate sample recent transactions
            recent_data = {
                'TX ID': ['TX-1023', 'TX-1022', 'TX-1021', 'TX-1020', 'TX-1019'],
                'Time': ['14:15:20', '14:15:20', '13:50:05', '13:30:12', '13:10:55'],
                'Amount': ['$2,450.00', '$120.50', '$4,600.00', '$25.00', '$99.99'],
                'Merchant': ['Electronics Retail', 'Grocery Store', 'Luxury Goods', 'Coffee Shop', 'Online Service'],
                'Device': ['Mobile (iOS)', 'Desktop (Windows)', 'Mobile (Android)', 'Mobile (iOS)', 'Desktop (Mac)'],
                'Risk Level': ['HIGH (92%)', 'LOW (12%)', 'MEDIUM (65%)', 'LOW (8%)', 'LOW (8%)']
            }
            
            df_recent = pd.DataFrame(recent_data)
            
            st.dataframe(
                df_recent,
                width="stretch",
                height=250,
                hide_index=True
            )

    else:  # Batch Analysis
        st.subheader("üìÅ Batch Analysis - Upload CSV")
        st.caption(
            "Batch Analysis simulates real-time, unlabeled transaction streams. Supervised metrics are shown only when labels are available."
        )
        demo_threshold = st.slider(
            "Sensitivity / Fraud Threshold (Demo)",
            min_value=0.30,
            max_value=0.90,
            value=0.50,
            step=0.05,
            help="Lower thresholds increase sensitivity (more alerts). Higher thresholds reduce alerts but may miss fraud.",
        )
        uploaded = st.file_uploader(
            "Upload CSV (include 'Class' for metrics like AUC-ROC / Precision / Recall / F1)",
            type=["csv"],
        )
        
        if uploaded is not None:
            with st.spinner("Processing batch data..."):
                df = pd.read_csv(uploaded)

                # Detect ground-truth label column (case-insensitive) for evaluation
                label_aliases = ["Class", "class", "label", "target", "is_fraud", "fraud", "y"]
                label_col = None
                cols_lower = {c.lower(): c for c in df.columns}
                for cand in label_aliases:
                    if cand in df.columns:
                        label_col = cand
                        break
                    if cand.lower() in cols_lower:
                        label_col = cols_lower[cand.lower()]
                        break
                
                df_fe = basic_feature_engineering(df)
                if feature_names is not None:
                    feature_cols = feature_names
                else:
                    feature_cols = [
                        c
                        for c in df_fe.columns
                        if c != (label_col or "Class") and np.issubdtype(df_fe[c].dtype, np.number)
                    ]
                for c in feature_cols:
                    if c not in df_fe.columns:
                        df_fe[c] = 0.0
                X = df_fe[feature_cols].values.astype(float)
                X_scaled = scaler.transform(X)
                probas = predict_fraud_proba(X_scaled)

                # In highly imbalanced fraud settings, well-calibrated probabilities can be small.
                # For a demo slider range of 0.30‚Äì0.90, use a batch-normalized risk score so the
                # user can see sensitivity changes even when raw probabilities are low.
                pmin = float(np.nanmin(probas)) if len(probas) else 0.0
                pmax = float(np.nanmax(probas)) if len(probas) else 0.0
                denom = max(pmax - pmin, 1e-9)
                risk_score = (probas - pmin) / denom
                risk_score = np.clip(risk_score, 0.0, 1.0)

                fraud_threshold_batch = float(demo_threshold)
                preds = (risk_score >= fraud_threshold_batch).astype(int)
                
                df['fraud_proba'] = probas
                df['risk_score'] = risk_score
                df['predicted_fraud'] = preds

                # -------------------------
                # Risk summary (primary)
                # -------------------------
                total_tx = len(df)
                high_risk_count = int(preds.sum())
                pct_high_risk = (high_risk_count / total_tx) if total_tx else 0.0
                avg_proba = float(np.nanmean(probas)) if total_tx else 0.0

                st.caption(
                    f"Fraud probability range in this batch: {pmin:.3f}‚Äì{pmax:.3f}. "
                    "The threshold slider is applied to a normalized risk score (0‚Äì1) to demonstrate sensitivity."
                )
                
                # -------------------------
                # Supervised evaluation (optional)
                # Only when: labels exist AND both classes exist
                # -------------------------
                y_true = None
                accuracy = None
                auc = None
                prec, rec, f1 = None, None, None
                cm = None
                supervised_ok = False
                supervised_reason = None

                if label_col is not None:
                    y_true_raw = pd.to_numeric(df[label_col], errors="coerce")
                    valid = y_true_raw.notna() & y_true_raw.isin([0, 1])
                    if valid.any():
                        y_eval = y_true_raw[valid].astype(int).values
                        unique = np.unique(y_eval)
                        if unique.size == 2:
                            supervised_ok = True
                            y_true = y_eval
                            probas_eval = probas[valid.values]
                            preds_eval = preds[valid.values]

                            from sklearn.metrics import (
                                roc_auc_score,
                                accuracy_score,
                                precision_recall_fscore_support,
                                confusion_matrix,
                            )

                            accuracy = accuracy_score(y_true, preds_eval)

                            # AUC must be computed on raw probabilities (not thresholded)
                            try:
                                auc = roc_auc_score(y_true, probas_eval)
                            except ValueError:
                                auc = float("nan")

                            prec, rec, f1, _ = precision_recall_fscore_support(
                                y_true,
                                preds_eval,
                                average="binary",
                                zero_division=0,
                            )
                            cm = confusion_matrix(y_true, preds_eval)
                        else:
                            supervised_reason = "single-class"
                    else:
                        supervised_reason = "invalid-labels"
                else:
                    supervised_reason = "unlabeled"

                def _fmt_pct01(v):
                    if v is None:
                        return "N/A"
                    try:
                        if np.isnan(v):
                            return "N/A"
                    except Exception:
                        pass
                    return f"{v * 100:.1f}%"

                def _fmt_float(v, digits=3):
                    if v is None:
                        return "N/A"
                    try:
                        if np.isnan(v):
                            return "N/A"
                    except Exception:
                        pass
                    return f"{v:.{digits}f}"
                
                # -------------------------
                # Risk Summary (primary)
                # -------------------------
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.markdown(f"""
                    <div class="metric-card">
                        <div class="metric-label">Total Transactions</div>
                        <div class="metric-value">{total_tx:,}</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col2:
                    st.markdown(f"""
                    <div class="metric-card">
                        <div class="metric-label" title="Based on the selected demo threshold.">High-Risk %</div>
                        <div class="metric-value">{pct_high_risk*100:.1f}%</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col3:
                    st.markdown(f"""
                    <div class="metric-card">
                        <div class="metric-label">Avg Fraud Probability</div>
                        <div class="metric-value">{_fmt_float(avg_proba, digits=3)}</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col4:
                    st.markdown(f"""
                    <div class="metric-card">
                        <div class="metric-label">High-Risk Transactions Flagged</div>
                        <div class="metric-value">{high_risk_count}</div>
                    </div>
                    """, unsafe_allow_html=True)

                if high_risk_count == 0:
                    st.info("No transactions exceed the selected threshold. Lower the threshold to increase sensitivity.")

                # Top-N highest-risk transactions (primary)
                st.subheader("Top High-Risk Transactions")
                top_n = min(20, len(df))
                top_df = df.sort_values("risk_score", ascending=False).head(top_n).copy()
                show_cols = []
                for c in ["Time", "Amount"]:
                    if c in top_df.columns:
                        show_cols.append(c)
                show_cols += ["fraud_proba", "risk_score", "predicted_fraud"]
                if label_col is not None and label_col in top_df.columns:
                    show_cols.append(label_col)
                st.dataframe(
                    top_df[show_cols].style.background_gradient(subset=["risk_score"], cmap="RdYlGn_r"),
                    width="stretch",
                    height=300,
                )

                # -------------------------
                # Supervised metrics (only when valid)
                # -------------------------
                if supervised_ok:
                    st.subheader("Supervised Evaluation (Labeled Batch)")
                    st.caption(
                        "Primary evaluation metrics for imbalanced fraud data: AUC-ROC and Recall. AUC is computed from raw probability scores."
                    )

                    scol1, scol2, scol3, scol4, scol5 = st.columns(5)
                    with scol1:
                        st.markdown(f"""
                        <div class="metric-card">
                            <div class="metric-label">Accuracy</div>
                            <div class="metric-value">{_fmt_pct01(accuracy)}</div>
                        </div>
                        """, unsafe_allow_html=True)
                    with scol2:
                        # If AUC is NaN (shouldn't happen if both classes exist), hide it.
                        auc_text = _fmt_float(auc, digits=3)
                        st.markdown(f"""
                        <div class="metric-card">
                            <div class="metric-label">AUC-ROC</div>
                            <div class="metric-value">{auc_text}</div>
                        </div>
                        """, unsafe_allow_html=True)
                    with scol3:
                        st.markdown(f"""
                        <div class="metric-card">
                            <div class="metric-label">Precision</div>
                            <div class="metric-value">{_fmt_pct01(prec)}</div>
                        </div>
                        """, unsafe_allow_html=True)
                    with scol4:
                        st.markdown(f"""
                        <div class="metric-card">
                            <div class="metric-label">Recall</div>
                            <div class="metric-value">{_fmt_pct01(rec)}</div>
                        </div>
                        """, unsafe_allow_html=True)
                    with scol5:
                        st.markdown(f"""
                        <div class="metric-card">
                            <div class="metric-label">F1</div>
                            <div class="metric-value">{_fmt_float(f1, digits=3)}</div>
                        </div>
                        """, unsafe_allow_html=True)

                    # Confusion Matrix only when both classes exist
                    if cm is not None:
                        st.subheader("Confusion Matrix")
                        fig_cm = px.imshow(
                            cm,
                            labels=dict(x="Predicted", y="Actual", color="Count"),
                            x=["Safe", "Fraud"],
                            y=["Safe", "Fraud"],
                            color_continuous_scale="Blues",
                            text_auto=True,
                        )
                        fig_cm.update_layout(
                            template="plotly_dark",
                            height=300,
                            paper_bgcolor="#1e2530",
                            plot_bgcolor="#141820",
                        )
                        st.plotly_chart(fig_cm, width="stretch")

                    if int(preds.sum()) == 0:
                        st.info(
                            "Precision/Recall/F1 are 0 because the current threshold flags no positives. "
                            "Lower the threshold to increase sensitivity."
                        )
                else:
                    st.info("Supervised metrics are not applicable for unlabeled or single-class batch data.")

                # Additional metrics when ground truth is available
                if y_true is not None:
                    colp, colr, colf, _colspacer = st.columns(4)
                    with colp:
                        st.markdown(f"""
                        <div class="metric-card">
                            <div class="metric-label">Precision</div>
                            <div class="metric-value">{_fmt_pct01(prec)}</div>
                        </div>
                        """, unsafe_allow_html=True)
                    with colr:
                        st.markdown(f"""
                        <div class="metric-card">
                            <div class="metric-label">Recall</div>
                            <div class="metric-value">{_fmt_pct01(rec)}</div>
                        </div>
                        """, unsafe_allow_html=True)
                    with colf:
                        st.markdown(f"""
                        <div class="metric-card">
                            <div class="metric-label">F1-Score</div>
                            <div class="metric-value">{_fmt_float(f1, digits=3)}</div>
                        </div>
                        """, unsafe_allow_html=True)

                    st.caption("Primary evaluation metrics for imbalanced fraud data: AUC-ROC and Recall.")
                else:
                    st.info("AUC not applicable for unlabeled batch data.")
                    st.caption("Upload a CSV with a ground-truth label column named 'Class' to compute AUC-ROC and Recall.")
                
                # Visualizations
                col_left, col_right = st.columns(2)
                
                with col_left:
                    st.subheader("Risk Flag Distribution")
                    fraud_dist = pd.DataFrame({
                        'Category': ['Low-Risk (below threshold)', 'High-Risk (flagged)'],
                        'Count': [(preds == 0).sum(), (preds == 1).sum()]
                    })
                    fig_dist = px.pie(
                        fraud_dist,
                        values='Count',
                        names='Category',
                        color='Category',
                        color_discrete_map={
                            'Low-Risk (below threshold)': '#48bb78',
                            'High-Risk (flagged)': '#e53e3e',
                        }
                    )
                    fig_dist.update_layout(
                        template='plotly_dark',
                        height=300,
                        margin=dict(l=20, r=20, t=20, b=20),
                        paper_bgcolor='#1e2530',
                        plot_bgcolor='#141820'
                    )
                    st.plotly_chart(fig_dist, width="stretch")
                
                with col_right:
                    st.subheader("Probability Distribution")
                    fig_hist = px.histogram(
                        df,
                        x='fraud_proba',
                        nbins=50,
                        color_discrete_sequence=['#a855f7']
                    )
                    fig_hist.update_layout(
                        template='plotly_dark',
                        height=300,
                        margin=dict(l=20, r=20, t=20, b=20),
                        paper_bgcolor='#1e2530',
                        plot_bgcolor='#141820',
                        xaxis_title="Fraud Probability",
                        yaxis_title="Count"
                    )
                    st.plotly_chart(fig_hist, width="stretch")
                
                # Scored Results Table
                st.subheader("Scored Transactions (Top 50)")
                display_cols = ['fraud_proba', 'risk_score', 'predicted_fraud']
                if 'Amount' in df.columns:
                    display_cols.insert(0, 'Amount')
                if 'Time' in df.columns:
                    display_cols.insert(0, 'Time')
                if label_col is not None and label_col in df.columns:
                    display_cols.append(label_col)
                
                st.dataframe(
                    df[display_cols].head(50).style.background_gradient(subset=['risk_score'], cmap='RdYlGn_r'),
                    width="stretch",
                    height=400
                )

    st.markdown("---")
    st.caption("‚ö†Ô∏è This system demonstrates a simulated real-time fraud detection pipeline for academic and research purposes.")


if __name__ == "__main__":
    main()
